<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
   "http://www.w3.org/TR/html4/loose.dtd">
<html> <head>
	<title> Chris&#39;s Wiki :: blog </title>
	<link href="/~cks/dwiki/dwiki.css" rel="stylesheet" type="text/css"> 
	<meta name="viewport" content="width=device-width">
	<link rel="alternate" type="application/atom+xml" href="/~cks/space/blog/?atom">
	
</head>
<body>
<div id="header">
	<div class="left"><a href="/~cks/">Chris Siebenmann</a> ::
<span class="breadcrumbs"><a href="/~cks/space/">CSpace</a> &raquo;
       blog</span></div>
	<div class="right">Welcome, guest.</div>
</div>

<div id="documentbody">
<h1 class="wttitle"> <a href="/~cks/space/blog/">Wandering Thoughts</a>  </h1>
<div class="wtblog">
<div class="maintext"> 
	<p class="daymarker" align=center><b>2023-03-09</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/linux/ZFSAndNFSMountInvalidation">ZFSAndNFSMountInvalidation</a> </h2> </div>
<div class="wikitext"><h2>ZFS on Linux and when you get stale NFSv3 mounts</h2>

<p>Suppose that you have <a href="/~cks/space/blog/linux/ZFSFileserverSetupIII">ZFS based NFS servers</a>
that you're changing from Ubuntu 18.04 to 22.04. These servers have
a lot of NFS exported filesystems that are mounted and used by a
lot of clients, so it would be very convenient if you could upgrade
the ZFS fileservers without having to unmount and remount the
filesystems on all of your clients. Conversely, if a particular way
of moving from 18.04 to 22.04 is going to require you to unmount
all of its filesystems, you'd like to know that in advance so you
can prepare for it, rather than find out after the fact when clients
start getting 'stale NFS handle' errors. Since we've just been
through some experiences with this, I'm going to write down what
we've observed.</p>

<p>There are at least three ways to move a ZFS fileserver from Ubuntu
18.04 to Ubuntu 22.04. I'll skip upgrading it in place because we
don't have any experience with that; <a href="/~cks/space/blog/linux/WhyNotInplaceOSUpgrades">we upgrade machines by
reinstalling them from scratch</a>. That
leaves two approaches for a ZFS server, which I will call a <em>forklift
upgrade</em> and a <em>migration</em>. In a forklift upgrade, you build new
system disks, then swap them in by exporting the ZFS pools, changing
system disks, booting your new 22.04 system, and importing the pools
back.</p>

<p>(As a version of the forklift upgrade you can reuse your current
system disks, although this means you can't readily revert.)</p>

<p>Our experience with these in place 'export pools, swap system disks,
import pools' forklift upgrades is that client NFSv3 mounts survive
over them. Your NFS clients will stall while your ZFS NFS server
goes away for a while, but once it's back (under the right host
name and IP address), they resume their activities and things pick
right back up where they were. We've also had no problems with ZFS
pools when we reboot our servers with changed hostnames; changing
the server's hostname doesn't cause ZFS on Linux to not bring the
pools up on boot.</p>

<p>However, forklift upgrades can only be done on ZFS fileservers where
you have separate system disks and ZFS pool disks. <a href="/~cks/space/blog/sysadmin/LocalVarMailImprovement">We have one
fileserver where this isn't possible</a>;
it has only four disks and shares all of them between system
filesystems and its ZFS pool. For this machine we did a <em>migration</em>,
where we built a new version of the system using new disks on new
hardware, then moved the ZFS data over with ZFS snapshots (<a href="/~cks/space/blog/sysadmin/UpgradingMachinesWithState">as I
thought we might have to</a>).
Once the data was migrated, we shut down the old server and made
the new hardware take over the name, IP address, and so on.</p>

<p>Unfortunately for us, when we did this migration, NFS clients got
stale NFS mounts. The new version of this fileserver had the same
filesystem with the exact same contents (ZFS snapshots and snapshot
replication insures that), the same exports, and so on, but <a href="/~cks/space/blog/unix/NFSFilehandleInternals">the
NFS filehandles</a> came out different.
It's possible that we could have worked around this if we had set
an explicit '<code>fsid=</code>' value in our NFS export for the filesystem
(as per <a href="https://man7.org/linux/man-pages/man5/exports.5.html"><code>exports(5)</code></a>), but it's
also possible that there were other differences in the NFS filehandle.</p>

<p>(ZFS has a notion of a 'fsid' and a 'guid' for ZFS filesystems
(okay, datasets), and zdb can in theory dump this information, but
right now I can't work out how to go from a filesystem name in a
pool to reading out its ZFS fsid, so I can't see if it's preserved
over ZFS snapshot replication or if the receiver generates a new
one.)</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/linux/ZFSAndNFSMountInvalidation?showcomments#comments">One comment</a>.)</div>
<small><a href="/~cks/space/blog/linux/ZFSAndNFSMountInvalidation">linux/ZFSAndNFSMountInvalidation</a> written at 22:38:51; <a href="/~cks/space/blog/linux/ZFSAndNFSMountInvalidation?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-03-08</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/linux/DebconfWhiptailVsXterm">DebconfWhiptailVsXterm</a> </h2> </div>
<div class="wikitext"><h2>Debconf's questions, or really whiptail, doesn't always work in xterms</h2>

<p>Every so often I install or upgrade a package by hand on one of
<a href="https://support.cs.toronto.edu/">our</a> Ubuntu servers and the
package stops to ask me questions, because <a href="/~cks/space/blog/linux/UbuntuUpdateProcessDislike">that is a thing that
Debian packages can do</a>. Usually this
is pretty close to fatal, because <a href="https://mastodon.social/@cks/109982648901372273">in my normal xterm environment,
the default interactive interface Debconf uses for this doesn't
work</a>. Specifically,
there is no way to see what the current selection theoretically is,
which leaves me flying blind in picking an answer.</p>

<p>The ultimate cause for this turns out to be that <strong>the <a href="https://manpages.debian.org/bullseye/whiptail/whiptail.1.en.html"><code>whiptail</code></a>
program doesn't work in an <a href="https://invisible-island.net/xterm/">xterm</a>
that has colour turned off</strong>. Whiptail is <a href="https://manpages.debian.org/bullseye/debconf-doc/debconf.7.en.html#Frontends">the default program
used for the default 'dialog' debconf frontend</a>
(<a href="https://kolektiva.social/@Anarcat/109982789634101272">thanks to @anarcat for telling me about this</a>). Contrary
to what I thought before I tried it, whiptail doesn't intrinsically
require colour, as it will work if you claim your xterm is, say, a
VT100 (with eg '<code>export TERM=vt100</code>'). The alternative <a href="https://manpages.debian.org/bullseye/dialog/dialog.1.en.html"><code>dialog</code></a>
program works fine if your xterm has had its colours forced off,
and <a href="https://manpages.debian.org/bullseye/debconf-doc/debconf.7.en.html#DEBCONF_FORCE_DIALOG">you can force debconf to use dialog instead of whiptail</a>.</p>

<p>(In a terminal environment that it thinks can do colour, whiptail relies
on colour to highlight your selection so you know what it is. If the
terminal is not actually displaying colour, this goes badly.)</p>

<p>Xterm is relatively unique in X terminal programs in that it supports
text colours but allows you to turn them off at runtime as a command
line option (or an X resource setting, <a href="https://mastodon.social/@cks/109982924464399854">which is what I use</a>). I disable terminal
colours whenever I can because they're almost always hard for me to
read, especially in the generally rather intense colour set that xterm
uses (<a href="/~cks/space/blog/unix/TerminalColoursNotTheSame">X terminal programs aren't consistent about what text colours
look like</a>, so the experiences of
people using Gnome Terminal are different here). Unfortunately, once
you've started xterm with colours off, as far as I know there's no way
to turn them back on.</p>

<p>(There is probably some escape sequences that can be used to query
xterm to see if it currently supports colours. I suspect that my odds
of getting the authors of <a href="https://manpages.debian.org/bullseye/whiptail/whiptail.1.en.html"><code>whiptail</code></a> to use them are functionally
zero.)</p>

<p>There are an assortment of manual workarounds, such as setting
various environment variables before running apt-get. The practical
problem is that, <a href="https://mastodon.social/@cks/109982824003312269">to quote myself from the Fediverse</a>:</p>

<blockquote><p>The broad problem is that Ubuntu and Debian package installs/updates
infrequently and irregularly ambush me with this and the default
configuration doesn't work. If I expect it I have many workarounds,
but generally I don't. And I'll never remember to always, 100% of the
time deploy the workarounds on all of our servers all of the time, no
matter what I'm doing.</p>
</blockquote>

<p>In theory debconf supports not even asking you questions, in the
form of <a href="https://manpages.debian.org/bullseye/debconf-doc/debconf.7.en.html#noninteractive">the <code>noninteractive</code> frontend</a>.
In practice I don't have enough confidence in Debian packages or
especially Ubuntu's version of them behaving sensibly when they're
forced into non-interactive mode. The very nature of being able to
ask questions means that people don't necessarily feel compelled
to make the default answer a sensible one.</p>

<p>Possibly the right answer for us is to deploy a general system
setting on our servers to prefer <a href="https://manpages.debian.org/bullseye/dialog/dialog.1.en.html"><code>dialog</code></a> over <a href="https://manpages.debian.org/bullseye/whiptail/whiptail.1.en.html"><code>whiptail</code></a>.
Unfortunately Ubuntu doesn't want you to remove the 'whiptail'
package itself; it's a dependency of the 'ubuntu-minimal' package,
and I don't really feel like finding out what effects stripping out
core looking 'ubuntu-&lt;etc>' packages have. Another option is for
me to configure xterm to set the '<code>$TERM</code>' environment variable to
'xterm-mono', which I expect exists on most Unix systems I'm likely
to use (or perhaps the older name 'xtermm', which is also on OpenBSD).
This version of xterm's <a href="https://man7.org/linux/man-pages/man5/terminfo.5.html">terminfo</a> capabilities
lacks colour entries entirely, and <a href="https://manpages.debian.org/bullseye/whiptail/whiptail.1.en.html"><code>whiptail</code></a> works fine with
it.</p>

<p>(I'm not intrinsically opposed to colours, but I am opposed to
blinding or hard to read colour choices, and a great deal of the
colours that programs try to use in terminal windows wind up that
way. The default colour set used by GNU Emacs for code highlighting
generally comes across to me as fairly nice, for example.)</p>

<p>PS: One way to see if your current terminal type claims to support
colours is '<code>tput colors</code>' (<a href="https://unix.stackexchange.com/a/10065">cf</a>).
In my regular xterms, this reports '8' (the basic number of ANSI
colours), while '<code>tput -T xterm-mono colors</code>' reports '-1', ie 'no'.</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/linux/DebconfWhiptailVsXterm?showcomments#comments">3 comments</a>.)</div>
<small><a href="/~cks/space/blog/linux/DebconfWhiptailVsXterm">linux/DebconfWhiptailVsXterm</a> written at 23:12:30; <a href="/~cks/space/blog/linux/DebconfWhiptailVsXterm?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-03-07</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/unix/TerminalColoursNotTheSame">TerminalColoursNotTheSame</a> </h2> </div>
<div class="wikitext"><h2>ANSI colours aren't consistent across X terminal programs</h2>

<p>There is a long standing set of '<a href="https://en.wikipedia.org/wiki/ANSI_escape_code#Colors">ANSI colour codes</a>' in terminal
emulators, including terminal programs for X. <a href="https://gist.github.com/JBlond/2fea43a3049b38287e5e9cefc87b2124">Here is a table of
them</a>,
and <a href="https://github.com/fidian/ansi">fidian/ansi</a> will provide you
with a convenient Bash script that will show you what these colors
look like in your terminal program. The latter is potentially
relevant because, shockingly, no two X terminal programs I've tried
render these ANSI colours exactly the same (between xterm, urxvt,
Gnome Terminal, and konsole; xfce4-terminal may render the same as
Gnome Terminal in some quick tests).</p>

<p>I've traditionally been very against using colours in my terminals,
because in my normal black on white choice, the colours programs
chose often wind up looking like an angry fruit salad explosion.
Given how glaringly annoying colours looked to me, I didn't really
understand why other people liked them until, a few years ago, I
noticed that the same colours in Gnome Terminal looked rather
different and generally came across as less of an assault on my
eyes. Using <a href="https://github.com/fidian/ansi">fidian/ansi</a> and its --color-table option shows that
I wasn't just imagining this; in side by side comparisons, Gnome
Terminal seems to clearly shift colours (in a black on white setup)
to less intense and more readable.</p>

<p>Beyond the colour shifts in Gnome Terminal, there are other interesting
colour changes from what you might expect. For instance, in all terminal
emulators, the result of rendering 'normal' white coloured text in a
black on white terminal is not invisible white text, but a greyish
colour that remains somewhat readable. There are also 'faint' versions
of basic ANSI colours, and the interpretation of faint white text on a
white background isn't necessarily what you'd expect and varies quite
a bit between terminal programs (with urxvt seeming to ignore the
faintness entirely for all colours).</p>

<p>With enough work I could probably find out what specific colours
Gnome Terminal is using and adjust my xterm to use them, so I have
less annoying colours on <a href="https://mastodon.social/@cks/109982924464399854">the rare occasions when I might need
them</a>. As a practical
matter, I'm not that interested in having colours in my terminals;
even most of Gnome Terminal's colours aren't all that appealing.</p>

<p>I'd like to put this forward as a reason for people to entirely
avoid using colours in terminal programs and terminal environments,
but I know that ship has sailed years ago. Apparently other people
have found or set up colour sets in their terminals that they like
and find a lot more readable than I do with any setup that I've
seen.</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/unix/TerminalColoursNotTheSame?showcomments#comments">6 comments</a>.)</div>
<small><a href="/~cks/space/blog/unix/TerminalColoursNotTheSame">unix/TerminalColoursNotTheSame</a> written at 21:33:42; <a href="/~cks/space/blog/unix/TerminalColoursNotTheSame?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-03-06</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/unix/OpenBSDPflogTcpdump">OpenBSDPflogTcpdump</a> </h2> </div>
<div class="wikitext"><h2>Special tcpdump filtering options for OpenBSD's pflog interface</h2>

<p>One of the convenient things that OpenBSD's <a href="https://man.openbsd.org/pf.4">pf packet filtering
system</a> can do is log packets of
interest, as covered in the <a href="https://man.openbsd.org/pf.conf#PACKET_FILTERING">Packet Filtering section of the pf.conf
manual page</a>.
These packets are logged to a special <a href="https://man.openbsd.org/pflog.4">pflog</a> network interface (where <a href="https://man.openbsd.org/pflogd.8">a
daemon</a> will generally write them
to disk). Since this is a network interface, you can monitor traffic
on it with OpenBSD's version of <a href="https://man.openbsd.org/tcpdump.8">tcpdump</a> (or use <a href="https://man.openbsd.org/tcpdump.8">tcpdump</a> to read the
log file).</p>

<p>As part of this, the OpenBSD tcpdump has some special additional
filtering options that are useful for selecting interesting traffic
on this pf logging interface. These are covered in <a href="https://man.openbsd.org/pcap-filter.5">pcap-filter</a>; many or all of them can
be found by searching for mentions of <a href="https://man.openbsd.org/pflog.4">pf(4)</a>. Here are the most notable ones
that I want to remember.</p>

<dl><dt><code>action &lt;something></code></dt>
<dd>Matches if PF blocked, passed, nat'd, or
did whatever to a particular packet. Using '<code>action block</code>' or
'<code>action pass</code>' can significantly reduce your confusion if you
have a mixture of pass and block pf rules that log traffic, as
we do. Because we have such a mix, I'm trying to condition myself
to always use 'action block' as part of tcpdump'ing pflog0.<p>
(For example, you might be passing and logging some traffic so
that you can see how much of it you have.)<p>
</dd>
<dt><code>inbound</code> or <code>outbound</code></dt>
<dd>I believe that these have the same
meaning as in <a href="/~cks/space/blog/unix/OpenBSDPfctlStates">pfctl -ss output</a>. If you
match in 'inbound' packets, you'll match only things logged by
'in' rules; if you match on 'outbound' packets, you'll match only
things logged by 'out' rules. Or at least you'll match when packets
are logged as they come in versus when they're logged as they get
sent out.<p>
</dd>
<dt><code>on &lt;interface></code></dt>
<dd>This matches packets that came from a specific
interface, regardless of what sort of rule caused them to be
logged.  With appropriate interface names, this may better
correspond to what you think of as 'inbound' or 'outbound'.<p>
</dd>
<dt><code>rnr &lt;number></code></dt>
<dd>This matches a specific rule number, but at
this point your life gets a little tricky because you have to find
out the number of the rule you want. The easiest way to do this is
to run '<code>pfctl -vv -s rules | grep @</code>' and then find your rule or
rules of interest. This also doesn't help you if the rule number
has changed from when the packet was logged (for example because
you've changed your pf.conf). You can use '<code>rulenum</code>' as a synonym
for this.<p>
(<a href="/~cks/space/blog/sysadmin/PfRulenumsAndTcpdump">At least things have gotten better here than they used to be
in 2011</a>.)</dd>
</dl>

<p>I believe that '<code>action block</code>' is pretty safe, but if you want
'everything but blocked' you may want to just use '<code>not action block</code>'
rather than trying to figure out which other actions your specific
configuration of rules needs you to use.</p>

<p>Our OpenBSD pflog0 interfaces appear to only log a relatively modest
amount of packet data; it's often not enough to do things like
completely reconstruct many DNS replies. I'm not sure how you increase
the packet size for pflog0 itself, unless it's controlled by the '-s
snaplen' argument of <a href="https://man.openbsd.org/pflogd.8">pflogd</a> (which
I initially read as controlling how much of the packet data from pflog0
would be saved to the log file).</p>
</div>
<small><a href="/~cks/space/blog/unix/OpenBSDPflogTcpdump">unix/OpenBSDPflogTcpdump</a> written at 22:02:40; <a href="/~cks/space/blog/unix/OpenBSDPflogTcpdump?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-03-05</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/programming/ChangesHaveContext">ChangesHaveContext</a> </h2> </div>
<div class="wikitext"><h2>An unexciting idea: Code changes have context</h2>

<p>I recently read Mark Dominus's <a href="https://blog.plover.com/2023/02/27/">I wish people would
stop insisting that Git branches are nothing but
refs</a> (<a href="https://lobste.rs/s/k5kgru/i_wish_people_would_stop_insisting_git">via</a>).
One of my thoughts afterward is that this feels like an instance of
a broader thing, which is that (code) changes have context; here,
one part of that context is where they happen (ie, what branch they
happen on). Of course we already know that in a sense, because Git
(and pretty much every other version control system) considers it
important to record both who made the change and when it was made.</p>

<p>In a way, it is turtles all of the way down. It's not too wrong to
say that in Git, the core objects are trees. Changes (commits) are
a record of the relationship between trees; they give you the context
of moving from one tree to another (often partially literally in
the form of the commit message and anything it points you to). Our
desire for this context is one reason people emphasize that you
should write good commit messages. In a sense, diffs themselves are
an expression of that context, since they are literally what changed
between the two (or more) trees involved (although <a href="/~cks/space/blog/programming/DiffsRequireContextII">diffs by
themselves aren't necessarily enough context</a>).</p>

<p>When we move one more level up, branches are one expression of the
context of changes (commits) themselves. Branches generally have some
sort of meaning, and they also represent (are) separate sequences
of changes; that separation adds context to the changes themselves,
although for more context you need to know what the branches are. Of
course branches aren't the only way of adding context to changes (there
are many ways of putting it into commit messages). Nor are they the only
context to changes we care about, since sometimes we care if particular
changes are in a release or in a version that someone is running.</p>

<p>(The question of 'has this change been merged into the main branch'
is an interesting edge case. Here, we do care about the state of a
change in the context of a branch, but it's not the branch the
change was initially created in. Knowing that the whole branch was
merged into the main branch would only be helpful if you knew that
the branch didn't continue on beyond that merge.)</p>

<p>A corollary to this is that you'll forget this context over time. This
makes me feel that it's worth putting as much of it as possible in a
durable and accessible form, which probably means the commit message
(since that's often the most accessible place). Code comments can
help, but they're only attached to the new state so it may take some
contortions to discuss the change. I've sometimes engaged in this
when I think it's important enough (or where I may not think to find
and look back at a commit message), but putting dates and discussions
of how the old state used to be in comments feels somewhat wrong.</p>

<p>(I suspect that all of this is obvious, but Mark Dominus's article
crystalized this in my mind so I feel like writing it down.)</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/programming/ChangesHaveContext?showcomments#comments">One comment</a>.)</div>
<small><a href="/~cks/space/blog/programming/ChangesHaveContext">programming/ChangesHaveContext</a> written at 22:51:23; <a href="/~cks/space/blog/programming/ChangesHaveContext?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-03-04</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/tech/SSDBlockDiscardHowSecure">SSDBlockDiscardHowSecure</a> </h2> </div>
<div class="wikitext"><h2>How secure is merely discarding (TRIMing) all of a SSD's blocks?</h2>

<p>Suppose hypothetically that you have some SSDs to securely dispose
of, and that for one reason or another you can't use <a href="https://ata.wiki.kernel.org/index.php/ATA_Secure_Erase">built-in SSD
secure erase</a>
on them, for example (apparently) because your BIOS automatically
locks out that option when it boots. You might wonder how well
protected you are if you simply tell the SSD to <a href="/~cks/space/blog/tech/SSDsAndBlockDiscardTrim">discard all of
its data</a>. Unsurprisingly, the answer is
that it depends.</p>

<p>First off, any SSD you want to use today will support what's called
'Deterministic Read After TRIM (DRAT)' (sort of <a href="https://en.wikipedia.org/wiki/Trim_(computing)#ATA">cf</a>), where the
SSD will always return a fixed result when you read data after a
TRIM operation. Some SSDs also promise to always return zeros in
this situation; this is 'Deterministic read ZEROs after TRIM',
variously abbreviated as 'DZAT', 'RZAT', or 'DRZAT'. These are the
(S)ATA versions, but NVMe has a similar system. All of these mean
that once you TRIM the entire drive, the previous data on the drive
can't be read through normal means, so someone who gets your drive
and puts it in a computer will get garbage (or <a href="https://superuser.com/questions/1652827/how-to-tell-whether-zeros-originate-from-trim-or-from-actually-writing-zeros-on">possibly errors
on NVMe drives</a>).</p>

<p>(My impression is that vendors initially supported DZAT only on
higher end (or at least more expensive) SSDs they sold for use in
RAID arrays, although support for this seems to have trickled down
to at least some modern consumer SSDs. Supporting DRAT but not DZAT
strikes me as mostly a market segmentation thing; if you're going
to be deterministic, making it always zero seems as easy as anything
else.)</p>

<p>If the drive then goes on to actually erase all of the flash blocks with
any copies of what you've TRIM'd, then as far as I know the data is
completely unrecoverable. Flash storage, unlike traditional hard drives,
can really be completely and irrecoverably erased, with no lingering
magnetic ghosts that a sufficiently determined person could in theory
reconstruct. However, SSDs don't particularly promise to actually erase
all of your blocks after you've TRIM'd them. Erasing blocks is a time
and power consuming activity, so while a SSD probably wants to keep a
pool of already erased blocks for new writes, it might not keep going
on at full pace once it thinks it has a big enough pool. SSDs make no
promises here and as far as I know there is no reliable, normal way to
tell how many erased blocks they have or if they've erased all your
blocks. Letting a TRIM'd SSD sit powered on but idle for minutes or
hours likely increases your chances that everything gets TRIM'd, but
doesn't guarantee it. There's also no certainty that a SSD will erase a
block that it's decided is too unreliable to reuse.</p>

<p>The lack of certainty on erasure matters because SSDs can be put
into a special <a href="https://blog.elcomsoft.com/2019/01/life-after-trim-using-factory-access-mode-for-imaging-ssd-drives/">factory mode</a>
that generally allows raw access to the flash storage and allows
you to stop the SSD from doing any further block erasure. If you
can put a drive into this state you can read out TRIM'd but not yet
erased blocks, although you may not know what logical blocks they
were. Serious data recovery companies can probably put pretty much
any SSD from any mainline maker into this recovery mode, which means
that anyone who wants to spend enough money can probably pull out
any not yet erased data from a TRIM'd drive. If they can't get the
pre-TRIM mapping of logical blocks to flash storage, making sense of
the result may take a lot of work but it's probably not impossible.</p>

<p>So on my taxonomy of <a href="/~cks/space/blog/tech/DiskErasingWhoAreYouStopping">who you're trying to stop when securely
erasing disks</a>, simply TRIM'ing your
SSDs definitely stops the basic threat of 'someone plugs it into
a computer and tries', but probably doesn't entirely stop the threat
of 'someone is willing to spend a bunch of money to send it to a
data recovery firm'. Letting your drives sit so that they erase
as many blocks as possible will make the life of the second sort
of person harder, but not impossible.</p>

<p>(TRIM'ing your SSD and then filling it up with new junk data will
probably help here, because it will push the drive to erase almost
everything. Randomly rewriting scattered bits afterward with more
junk will probably push the drive into erasing its overprovisioned
blocks too. But all of that is a speculative guess, because SSDs
are black boxes. If this matters a lot to you, you want to use
SSDs that have good implementations of secure erase. How you find
out what those SSDs are, I don't know.)</p>

<p>(Under Linux, you can use '<code>hdparm -I</code>' to see what SATA SSDs support
(or claim to), and <a href="https://unix.stackexchange.com/questions/472211/list-features-of-nvme-drive-like-hdparm-i-for-non-nvme">see this stackexchange question and answer for
how to do it on NVMe drives</a>.)</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/tech/SSDBlockDiscardHowSecure?showcomments#comments">One comment</a>.)</div>
<small><a href="/~cks/space/blog/tech/SSDBlockDiscardHowSecure">tech/SSDBlockDiscardHowSecure</a> written at 22:48:35; <a href="/~cks/space/blog/tech/SSDBlockDiscardHowSecure?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-03-03</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/tech/DiskErasingWhoAreYouStopping">DiskErasingWhoAreYouStopping</a> </h2> </div>
<div class="wikitext"><h2>When securely erasing disks, who are you trying to stop?</h2>

<p>Various people, <a href="https://support.cs.toronto.edu/">us</a> included,
periodically have the need to securely dispose of disk drives that
we no longer need or want, where by 'securely' we mean that people
shouldn't be able to get our data from the drives after we've gotten
rid of them.  Often there are questions of what you need or want
to be doing in order to achieve this security. In my view, part of
the answer to this is depends on who you want to stop from getting
your data (and how many resources you think they have).</p>

<p>(I'm not sure this should be called a <a href="https://en.wikipedia.org/wiki/Threat_model">threat model</a>, but it's the same
sort of general idea.)</p>

<p>So here is my take on multiple levels of threat you might face,
from the most common to the rarest (assuming that you're starting
from working drives before you begin disposing them).</p>

<ol><li>Someone who gets their hands on your disks (either buying them
second hand or picking them up from somewhere), sticks them in a
computer, and tries to read them. This used to happen all of the
time; people would buy surplus disk drives (or entire computers)
from eBay, plug them in, and all sorts of sensitive data would
come flying out.<p>
If this happens today people will be very upset at you, and for
good reasons, because this is basically 'Hardware Disposal 101'.
Everyone should know you can't just turn computers off then toss
them out the door; you have to do something to make it so your
data doesn't leak out with them.<p>
</li>
<li>People who can put drives into <a href="https://blog.elcomsoft.com/2019/01/life-after-trim-using-factory-access-mode-for-imaging-ssd-drives/">a special factory mode</a>
that allows access to low-level data reading commands. On SSDs
this will probably allow them access to reserved space and <a href="/~cks/space/blog/tech/SSDsAndBlockDiscardTrim">blocks
that have been discarded but not yet erased</a>.<p>
I believe that at least some data recovery services have this
capability, so you're also effectively worrying about people who
can send your old drives to a data recovery service and talk them
into having a go at it. Thus, you should probably assume that any
actual attacker is at this level (as opposed to people who just
picked up some of your drives and are curious what they'll see).<p>
In general, data recovery services go some way to making it so
that an attacker mostly needs money (and your drives) instead of
good technical capabilities. Attackers can to some extent outsource
the technical expertise, assuming they can find a suitable firm
and the firm is willing to work for them on your drives.<p>
</li>
<li>People who can load custom firmware onto drives, giving them at
least as much access as the most powerful factory mode (regardless
of what the drive's normal factory mode supports). These people
can definitely read all of the raw storage (flash or spinning
rust) and otherwise exert very low level control over the drive.
Sometimes or a lot of the time the drive's standard factory mode
will make loading your own firmware unnecessary, so this may
basically be the same as the previous level.<p>
</li>
<li>People who can directly read data from any intact physical storage,
either flash chips or hard drive platters. These people (probably)
don't need the controller to be intact and operable, so even
physical damage or destruction of it alone isn't enough. For
example, these people wouldn't be stopped if you drilled holes in
a SSD's PCB and snapped it apart, as long as the flash chips are
still fine.<p>
</li>
<li>People who can directly read (some) data even from partially
damaged physical storage, such as drilled (or snapped) hard drive
platters or a partially damaged flash chip. To stop these people
you need either complete physical destruction or for the data
that's on the storage to be useless, for example because it's
encrypted and you've destroyed the encryption keys.</li>
</ol>

<p>(There is a related dimension of how much repair people can do to disk
drives that you've deliberately damaged. A data recovery firm that's
given a decent amount of money might be able to repair moderate damage,
like a snapped SSD PCB, and then go on to recover data from it.)</p>

<p>As mentioned, not stopping the first sort of people from getting
at your data is basic negligence by this point. You absolutely have
to do that much. On the other extreme, against the last level of
people any method of destroying a disk drive that isn't using a
feature specifically designed to securely erase its data is probably
not good enough. On the other hand, you're probably not going to be
targeted by such people, and if you are being targeted by them the
<a href="https://www.usenix.org/system/files/1401_08-12_mickens.pdf">Mickens 'Mossad' rule</a> (<a href="https://www.schneier.com/blog/archives/2015/08/mickens_on_secu.html">also</a>)
probably applies.</p>

<p>Modern SSDs have <a href="https://ata.wiki.kernel.org/index.php/ATA_Secure_Erase">(S)ATA secure erase</a> (<a href="https://www.thomas-krenn.com/en/wiki/Perform_a_SSD_Secure_Erase">also</a>) and
<a href="https://wiki.archlinux.org/title/Solid_state_drive/Memory_cell_clearing#NVMe_drive">NVMe secure erase</a>
features that, if implemented properly, will normally protect you
against everyone. As mentioned, the most certain approach is competent
host level encryption where you do your best to totally destroy the
real underlying encryption keys (<a href="/~cks/space/blog/tech/DiskEncryptionAndKeying">which haven't always been the
keys you enter yourself</a>), and then probably
you also do a SSD level secure erase. However, all of this requires
the drive to be in working order; if the drive has failed already
and you're worried about someone bringing it back to life and getting
your data, you may have a problem (although host level encryption may
still save you).</p>

<p>PS: As far as I know, once a SSD has erased a given flash block,
the data in that block is irretrievably gone (<a href="https://www.electronics-notes.com/articles/electronic_components/semiconductor-ic-memory/how-flash-memory-works-operation.php">cf</a>,
<a href="https://en.wikipedia.org/wiki/Flash_memory#Writing_and_erasing">also</a>).
This is different from (some) hard drive technologies, where magnetic
echos of old data could remain potentially detectable even after a
sector had been rewritten.</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/tech/DiskErasingWhoAreYouStopping?showcomments#comments">5 comments</a>.)</div>
<small><a href="/~cks/space/blog/tech/DiskErasingWhoAreYouStopping">tech/DiskErasingWhoAreYouStopping</a> written at 22:17:38; <a href="/~cks/space/blog/tech/DiskErasingWhoAreYouStopping?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-03-02</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/sysadmin/ModernEmailAddressesUTF8">ModernEmailAddressesUTF8</a> </h2> </div>
<div class="wikitext"><h2>Modern email addresses can be in UTF-8</h2>

<p>Over on the Fediverse, <a href="https://mastodon.social/@cks/109955281837510854">I noted</a>:</p>

<blockquote><p>It has been '0' days since someone's email client helpfully let them
use a Unicode '‚Äê' instead of an ASCII '-' in dash-separated email
addresses. Or perhaps the client automatically used the Unicode
character instead of the ASCII dash.</p>

<p>You may not be surprised to hear that email systems, ours included,
don't consider the two to be the same. I'm not sure how it even works,
although some sending MTAs appear to just send the address as UTF-8.</p>
</blockquote>

<p>Specifically, the character in question is <a href="https://codepoints.net/U+2010?lang=en">Unicode U+2010 Hyphen</a> (<a href="https://util.unicode.org/UnicodeJsps/character.jsp?a=2010">also</a>). The
email in question was sent to us using this character in a destination
address that actually had the ASCII dash; given that the U+2010
version of the address didn't exist, Exim on our external MX gateway
rejected it. These days, Exim's logging is in UTF-8, as is pretty
much anything you'll use to read the logs, so the result was pretty
confusing to disentangle. To all appearances it looked like our
email system had temporarily glitched out and decided that some
valid local addresses didn't actually exist.</p>

<p>The answer to my final question, about how this actually works, is
<a href="https://www.rfc-editor.org/rfc/rfc6531">RFC 6531: SMTP Extension for Internationalized Email</a>, also known as SMTPUTF8.
<a href="https://www.exim.org/exim-html-current/doc/html/spec_html/ch-internationalisation.html">Exim supports SMTPUTF8</a>
(if built appropriately), and it defaults to advertising this to
everyone (per <a href="https://www.exim.org/exim-html-current/doc/html/spec_html/ch-main_configuration.html">Main configuration</a>
and the description of smtputf8_advertise_hosts in it). To
simplify, a large part of what SMTPUTF8 support does is that the
sender can use UTF-8 in envelope addresses, both MAIL FROM and RCPT
TO. Either or both of the local part and the (sub)domain can be in
UTF-8, although the resulting DNS label needs to conform with <a href="https://en.wikipedia.org/wiki/Internationalized_domain_name">IDNA</a>.</p>

<p>Allowing email addresses to use U+2010 hyphens instead of ASCII
ones is a trivial use of SMTPUTF8. A potentially much more important
one for genuine internationalization is allowing people to have
addresses that aren't written only in ASCII, for example because
their name itself is not ASCII. Any number of Europeans have accented
characters in their names and so might like to have them in their
email addresses, and then there's quite a lot of people who don't
write their names in any version of the Latin alphabet. SMTPUTF8
accommodates all of them.</p>

<p>Of course not all mail systems out there in the world support SMTPUTF8,
so today anyone using such an email address is taking some degree
of risk (unless their system automatically handles the situation of
a destination mail server not supporting SMTPUTF8 by, for example,
rewriting the envelope address and possibly message headers to a known
alternate version). But I suspect that the large email providers all
support it, and their support for it (and willingness to generate and
use email addresses in UTF-8) will push everyone to support it sooner
or later.</p>

<p>(I have actually encountered SMTPUTF8 before, <a href="/~cks/space/blog/programming/CodeCommentsWhy">cf</a>, but in the time since then I
forgot about it.)</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/sysadmin/ModernEmailAddressesUTF8?showcomments#comments">3 comments</a>.)</div>
<small><a href="/~cks/space/blog/sysadmin/ModernEmailAddressesUTF8">sysadmin/ModernEmailAddressesUTF8</a> written at 23:06:06; <a href="/~cks/space/blog/sysadmin/ModernEmailAddressesUTF8?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-03-01</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/linux/SystemdDynamicUserNFSAndGroups">SystemdDynamicUserNFSAndGroups</a> </h2> </div>
<div class="wikitext"><h2>A gotcha with Systemd's <code>DynamicUser</code>, supplementary groups, and NFS (v3)</h2>

<p>Today <a href="https://mastodon.social/@cks/109950128454996029">I used bind mounts in an odd way</a>:</p>

<blockquote><p>Today's crazy Linux bind mount usage: <br>
# cp [-a] /a/nfs/mount/special-file /root/special-file <br>
# mount --bind /root/special-file /a/nfs/mount/special-file</p>

<p>This was the easiest way to make a systemd service with
DynamicUser=yes and a supplementary group get access to special-file,
which is only accessible by said group. (The normal version of the
service runs with the file not on NFS.)</p>

<p>I assume something about filesystem visibility for systemd dynamic
users but meh, life is short and I have a hammer.</p>
</blockquote>

<p>(Oops, I see I left out a critical '-a' cp argument in my initial
Fediverse post, <a href="https://mastodon.social/@cks/109951546087080567">cf</a>.)</p>

<p>Linux's bind mounts are normally used with directories, but it's
equally valid to bind mount a file, as I'm doing here. By bind-mounting
the special file the service needs to access to a local file, I'm
taking NFS out of the picture. This turned out to be the right
answer (and in fact the only good one), but not for the reasons
that I thought.</p>

<p>This particular service uses <a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#DynamicUser="><code>DynamicUser=yes</code></a>
because it's a combination of <a href="/~cks/space/blog/linux/SystemdDynamicUserLike">more convenient and more secure</a>. Because things run by the service need
to read a private file, the service also has <a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#SupplementaryGroups=">a supplementary group</a>;
the private file is in the group, and the service has access to the
group. In the production deployment, this file lives on a local
filesystem; here, I was running a test setup, where having it on
NFS is more convenient. At first, I assumed that <a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#DynamicUser="><code>DynamicUser=yes</code></a>
was manipulating NFS mount related things so that the supplementary
group was ignored (it wasn't completely blocking NFS mount access,
because other things the service was using came from the same NFS
mount), but this isn't the problem. Instead, the problem is on <a href="/~cks/space/blog/linux/ZFSFileserverSetupIII">our
Linux NFS servers</a>.</p>

<p>Like many other people, our Linux NFS servers are configured to
allow people to (meaningfully) be in more than 16 groups, which is
<a href="/~cks/space/blog/unix/GroupLimitState">the NFS v3 protocol limit</a>. On Linux
NFSv3 servers, how this works is that <a href="/~cks/space/blog/linux/NFSFlushingServerGroupCache">the NFS server throws away
the group list from the NFS client and does its own local lookup</a>. We have a synchronized password file,
so for regular logins and groups the NFS servers have the same UIDs
and GIDs as the NFS clients (including for the supplemental group
used here) and this all works out. However, when you set
<a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#DynamicUser="><code>DynamicUser=yes</code></a>, systemd makes up a new UID (and GID) that
doesn't exist in your local /etc/passwd and so won't exist in the
NFS server's /etc/passwd either. When a process in the service makes
NFS requests, the NFS server takes the carefully curated list of
supplemental groups you set up in systemd, throws them away, looks
up the UID in its own /etc/passwd and /etc/group, finds nothing,
and concludes that this request has no group permissions at all.</p>

<p>(Indeed, now that I look I can see the telltale '&lt;uid> 0:' line in
the NFS server's /proc/net/rpc/auth.unix.gid/content, <a href="/~cks/space/blog/linux/NFSServerUsingGroupCache">cf</a>. Along with a few other unknown UIDs
that we're seeing from somewhere.)</p>

<p>When I used a bind mount to make the special file a local file, not
a NFS file, I bypassed the NFS server and with that, the NFS server
ignoring the local supplemental group. Once all of the access control
for the file was being done locally, by the client's kernel, the
supplemental group worked to allow access. I believe this was the
only way to solve the problem without changing the service unit.</p>

<p>So the end moral is <strong>supplemental groups don't work over NFSv3 with
systemd dynamic users</strong>. More generally, supplemental groups with
anonymous UIDs don't work over NFS; systemd dynamic users are merely one
way to get anonymous UIDs. For our uses this isn't a fatal problem, but
I'll want to remember it for the future.</p>

<p>(The workaround would be to allocate an actual UID for this purpose,
set it in the systemd unit file, and then possibly duplicate all of
the additional things that <a href="https://www.freedesktop.org/software/systemd/man/systemd.exec.html#DynamicUser="><code>DynamicUser=yes</code></a> normally does that
increase security and isolation.)</p>

<p>(I realized <a href="https://mastodon.social/@cks/109950562210109751">what the answer was</a> due to <a href="https://mas.to/@srtcd424/109950531058623298">a
suggestion from Steven Reid</a>.)</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/linux/SystemdDynamicUserNFSAndGroups?showcomments#comments">One comment</a>.)</div>
<small><a href="/~cks/space/blog/linux/SystemdDynamicUserNFSAndGroups">linux/SystemdDynamicUserNFSAndGroups</a> written at 22:27:02; <a href="/~cks/space/blog/linux/SystemdDynamicUserNFSAndGroups?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>
<p class="daymarker" align=center><b>2023-02-28</b></p>
<div class="blogentry">
<div class="entryid"> <h2> <a href="/~cks/space/blog/linux/UbuntuCanonicalProduct">UbuntuCanonicalProduct</a> </h2> </div>
<div class="wikitext"><h2>Ubuntu is a Canonical product</h2>

<p>A while back I wrote that <a href="/~cks/space/blog/linux/UbuntuIsCanonical">from an outside perspective, Ubuntu is
Canonical's thing</a>, in that Canonical runs the
show despite having outside contributors. But in the wake of
<a href="https://mastodon.social/@cks/109944814025438841">wrestling with Canonical's advertisements in a stock 22.04 LTS
machine and losing</a>,
I want to amend that observation with an important additional one.
Ubuntu is not merely Canonical's, Ubuntu is a Canonical product.
Which is to say, <strong>Ubuntu exists to make money for Canonical</strong>.
Further, the current evidence suggests that Canonical feels it's
not making enough money for them; hence the steadily increasing
advertisements in Ubuntu, <a href="https://www.theregister.com/2023/02/23/ubuntu_remixes_drop_flatpak/">along with other moves</a>.</p>

<p>Broadly speaking, we've seen this show before, most recently with
Red Hat/IBM and CentOS, so we can make some guesses about where
this version will go. If Canonical is now making enough money from
Ubuntu, they might stop here, with annoying things in your message
of the day and so on. Otherwise, they will definitely take additional
steps to make more money, and they probably have a number of those.
Would Canonical reduce the free LTS support interval from five years
to two and a half years? Perhaps. And fundamentally Canonical is
unlikely to be that interested in the views of people who have
little or no chance of giving them money, people like <a href="https://support.cs.toronto.edu/">us</a>.</p>

<p>(A shortened free LTS support period wouldn't be the death knell of
personal use of Ubuntu LTS, since Canonical currently gives free
personal use licenses for their paid extra support.)</p>

<p>The good news is that the sky isn't falling today; there's no particular
need to move away from Ubuntu for current or future use.  The other good
news is that because Ubuntu is so close to Debian, it will probably
be pretty easy to move to using Debian for future machines if the sky
does fall in. I'd expect almost all of <a href="/~cks/space/blog/linux/UbuntuOurInstallSystem">our local customizations to
the Ubuntu server installs</a> to drop right
in on top of Debian. The one area that will be different is the
installer itself, since <a href="/~cks/space/blog/linux/Ubuntu2004AutoinstFormat">Ubuntu uses a new installer since 20.04</a>.</p>

<p>(Energetic and concerned people might thus start building out a Debian
installer environment, or at least explore it to build up their
knowledge.)</p>

<p>Locally, <a href="https://support.cs.toronto.edu/">we</a>'re unlikely to
migrate away from Ubuntu LTS until we're forced to, because we
continue to like the predictable release schedule and five years
of support. However, I expect we'll be keeping in contact with
anyone else around here who's switched over to Debian, so we can
find out how they feel about the shift.</p>

<p>PS: The other thing that can happen with commercial products is
that they stop being made (or they get sold and drastically
transformed). On the sale front, I can imagine a future where Ubuntu
becomes, say, 'AWS Ubuntu' after Amazon buys out Canonical at a
suitably low price.</p>
</div><div class="commenttools">(<a href="/~cks/space/blog/linux/UbuntuCanonicalProduct?showcomments#comments">8 comments</a>.)</div>
<small><a href="/~cks/space/blog/linux/UbuntuCanonicalProduct">linux/UbuntuCanonicalProduct</a> written at 22:05:28; <a href="/~cks/space/blog/linux/UbuntuCanonicalProduct?writecomment" rel="nofollow">Add Comment</a></small>
</div>
<p> </p>

<div class="seemore">
<strong>(<a href="/~cks/space/blog/range/11-20/" rel="nofollow">Previous 10</a> or go back to <a href="/~cks/space/blog/2023/02/">February</a> <a href="/~cks/space/blog/2023/">2023</a> at <a href="/~cks/space/blog/2023/02/27/">2023/02/27</a>)</strong>
</div>
</div>
<div class="sidebar">
	<div class="readme"> <div class="wikitext"><p>These are my <a href="/~cks/space/blog/">WanderingThoughts</a> <br>
(<a href="/~cks/space/AboutBlog">About the blog</a>)</p>

<p><a href="/~cks/space/blog/__Index">Full index of entries</a> <br>
<a href="/~cks/space/blog/__RecentComments">Recent comments</a></p>

<p>This is part of <a href="/~cks/space/FrontPage">CSpace</a>, and is written by <a href="/~cks/space/People/ChrisSiebenmann">ChrisSiebenmann</a>. <br>
Mastodon: <a href="https://mastodon.social/@cks">@cks</a> <br>
<strike>Twitter</strike>: <a href="https://twitter.com/thatcks/">@thatcks</a></p>

<p align="center">* * *</p>

<p>Categories: <a href="/~cks/space/blog/links/">links</a>, <a href="/~cks/space/blog/linux/">linux</a>, <a href="/~cks/space/blog/programming/">programming</a>, <a href="/~cks/space/blog/python/">python</a>, <a href="/~cks/space/blog/snark/">snark</a>, <a href="/~cks/space/blog/solaris/">solaris</a>, <a href="/~cks/space/blog/spam/">spam</a>, <a href="/~cks/space/blog/sysadmin/">sysadmin</a>, <a href="/~cks/space/blog/tech/">tech</a>, <a href="/~cks/space/blog/unix/">unix</a>, <a href="/~cks/space/blog/web/">web</a> <br>
Also: <a href="/~cks/space/blog/__Topics">(Sub)topics</a></p>

<p>This is a <a href="/~cks/space/dwiki/DWiki">DWiki</a>. <br>
<a href="/~cks/space/help/GettingAround">GettingAround</a> <br>
(<a href="/~cks/space/help/Help">Help</a>)</p>
</div> </div>
<div class="sidesearch" style="font-size: small"> <form method=get action="/~cks/space/">Search: <input name=search size=15></form> </div>
</div>
</div>
 
</div>

<hr> Page tools: <a href="/~cks/space/blog/?normal" rel="nofollow">See As Normal</a>. 
<div class="bottombar">
	<div class="left"><form method=get action="/~cks/space/">Search: <input name=search size=15></form></div>
	<div class="right"><form method=post action="/~cks/space/.login">
Login: <input name=login size=10>
Password: <input type=password name=password size=10>
<input type=hidden name=view value=login>
<input type=hidden name=page value="blog">
<input type=submit value="Login"></form></div> <div class="clear"></div>
</div>
<div id="atomfeeds">Atom Syndication: <a type="application/atom+xml" href="/~cks/space/blog/?atom" rel="nofollow">Recent Pages</a>, <a type="application/atom+xml" href="/~cks/space/blog/?atomcomments" rel="nofollow">Recent Comments</a>.</div>

<hr> 
<address>This dinky wiki is brought to you by the Insane Hackers
Guild, Python sub-branch.</address>
</body>
</html>
